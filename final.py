# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PFuusi4kHpPZbgjSMtGyeHjb7AwDDG6m

## importing important libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
# %matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model  import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

"""## importing the data set"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Algerian_forest_fires_dataset.csv")
df.head()

df.tail()

"""## Data Cleaning and Preparation"""

df.columns

"""We can see that some the column names have extra spaces. Need to get rid of those"""

df.columns = df.columns.str.strip()
df.columns

df.shape

df.info()

df.isnull().sum()

df[df.isnull().any(axis = 1)]

"""Here the Missing values at 122 and 123th index seprate the data set in two regions.
* Bejaia region
* Sidi Bel-Abbes region.

We can make a new column as "Region" to separately identify the regions.
We will set Bejaia as 1 and Sidi Bel-Abbes as 2
"""

df['Region'] = 1

for i in range(len(df)):
  if i >= 122:
    df['Region'][i] = 2

"""Droping the NaN values"""

df = df.dropna().reset_index(drop = True)

df.isnull().sum()

df.value_counts('Classes')

"""More than two classes. Need further investigation"""

df['Classes'].unique()

"""We can see that some values have extra sapecs. Thats why it was showing more classes than it should be."""

df['Classes'] = df['Classes'].str.strip()
df['Classes'].unique()

"""There is a class name 'Classes'. Why is that?"""

df[~df.Classes.isin(['fire','not fire'])]

"""May be it was created when they merged data from two region together. Filtering out unnecessary class."""

df = df[df.Classes.isin(['fire','not fire'])]

df['Classes'].unique()

df.info()

"""Need to change the data types for the respective features for the analysis"""

df1 = df.copy()

df1 = df1.astype({
                'RH':np.int64, 'Temperature':np.int64,
                'Ws':np.int64, 'Rain':np.float64,
                'FFMC':np.float64 ,'DMC':np.float64,
                'DC':np.float64, 'ISI':np.float64,
                'BUI':np.float64, 'FWI':np.float64
                })

"""Encoding Not fire as 0 and Fire as 1"""

# df['Classes']= np.where(df['Classes']== 'not fire',0,1)
# df.head()

df1.to_csv('forests_fires.csv', index=False)

"""## Exploratory Data Analysis"""

df1.describe(include='all')

df1.info()

numeric_col = [col for col in df1.columns if df1[col].dtype != 'object']
object_col = [col for col in df1.columns if df1[col].dtype == 'object']

plt.style.use('ggplot')
plt.figure(figsize=(12, 8))
sns.boxplot(data=df1[numeric_col])
plt.title('Distribution of Features')
plt.show()

plt.style.use('ggplot')
plt.figure(figsize=(7, 5))
sns.countplot(data= df1 , x='Classes')
plt.title('Class distribution', fontsize = 14)
plt.show()

plt.style.use('ggplot')
plt.figure(figsize=(12, 5))
sns.countplot(data= df1 , x='month', hue='Classes')
plt.title('Forest Fire Analysis \nMonth wise', fontsize = 14)
plt.xticks(np.arange(4), ['June','July', 'August', 'September',])
plt.show()

plt.style.use('ggplot')
plt.figure(figsize=(12, 5))
sns.countplot(data= df1[df1['Region'] == 1] , x='month', hue='Classes')
plt.title('Forest Fire Analysis \nMonth wise in Bejaia Region', fontsize = 14)
plt.xticks(np.arange(4), ['June','July', 'August', 'September',])
plt.show()

plt.style.use('ggplot')
plt.figure(figsize=(12, 5))
sns.countplot(data= df1[df1['Region'] == 2] , x='month', hue='Classes')
plt.title('Forest Fire Analysis \nMonth wise in Sidi Bel-Abbes Region Region', fontsize = 14)
plt.xticks(np.arange(4), ['June','July', 'August', 'September',])
plt.show()

plt.style.use('ggplot')
plt.figure(figsize=(15, 12))
plt.suptitle('Bar Plot \nRelationship Between Target and Independent variables'
, fontsize=20, alpha=1, y=1.05)
for i in range(0, len(numeric_col)):
  plt.subplot(6,3,i+1)
  sns.barplot(x='Classes',
              y=numeric_col[i],
              data=df1)
  plt.tight_layout()

df1.corr()

plt.style.use('ggplot')
plt.figure(figsize=(12, 8))
sns.heatmap(df1.corr(), annot=True)
plt.title('Correlation among numerical features')
plt.show()

plt.style.use('ggplot')
plt.figure(figsize=(12, 8))
sns.histplot(data = df1, x= 'Temperature', hue='Classes', kde = True)
plt.title('Distribution of Temperature with respect to classes')
plt.show()

"""##Plotting Pie chart for percentage of fire or not fire on the data set"""

# Percentage for PieChart
percentage = df1.Classes.value_counts(normalize=True)*100
percentage

#plotting PieChart
classeslabels = ["FIRE", "NOT FIRE"]
plt.figure(figsize =(12, 7))
plt.pie(percentage,labels = classeslabels,autopct='%1.1f%%')
plt.title ("Pie Chart of Classes", fontsize = 15)
plt.show()

"""## Modeling : Classification

### Data preparation for modeling and Spliting the data into Training and Test set

#### Spliting the dataset into train and test
"""

df2 = df1.drop(['day', 'month', 'year'], axis = 1)
df2.head()

X = df2[['Temperature', 'RH', 'Ws', 'Rain', 'FFMC', 'DMC', 'DC', 'ISI', 'BUI',
       'FWI']]
y = df2['Classes']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

"""#### Scaling Features"""

def standard_scaler(X_train, X_test):
  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)
  X_test_scaled = scaler.transform(X_test)
  return X_train_scaled, X_test_scaled

X_train_scaled, X_test_scaled = standard_scaler(X_train, X_test)

plt.style.use('ggplot')
plt.figure(figsize=(12, 8))
sns.boxplot(data= X_train_scaled)
plt.title('Distribution of Features after Scaling')
plt.show()

"""#### **Logistic Regression**"""

from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Assuming X_train, X_test, y_train, y_test are your training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=702)

# Assuming model is your trained model, replace it with your actual model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Obtain the classification report
class_report = classification_report(y_test, y_pred)

# Print the classification report
print("Classification Report is:\n", class_report)

# Calculate and print the training score
training_score = model.score(X_train, y_train) * 100
print("\nTraining Score:\n", training_score)

LR_Prediction = model.predict(X_test)
LR_predicted_df = pd.DataFrame({'Actual': y_test, 'Predicted ': LR_Prediction})
LR_predicted_df.head(10)

"""#### Decision Tree Classifier"""

from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Assuming X and y are your features and labels
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1002)

# Assuming clf is your classifier (e.g., Decision Tree)
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Generate a classification report
class_report = classification_report(y_test, y_pred)
print(f"Classification Report is:\n{class_report}")

# Calculate and print the training score
training_score = clf.score(X_train, y_train) * 100
print(f"\nTraining Score:\n{training_score:.1f}")

DT_Prediction = clf.predict(X_test)
DT_predicted_df = pd.DataFrame({'Actual': y_test, 'Predicted ': DT_Prediction})
DT_predicted_df.head(10)

"""#### Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Assuming X and y are your features and labels
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=402)

# Assuming clf is your classifier (Random Forest)
RF_clf = RandomForestClassifier()
RF_clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Generate a classification report
class_report = classification_report(y_test, y_pred)
print(f"Classification Report is:\n{class_report}")

# Calculate and print the training score
training_score = clf.score(X_train, y_train) * 100
print(f"\nTraining Score:\n{training_score:.1f}")

RF_Prediction = RF_clf.predict(X_test)
 RF_predicted_df = pd.DataFrame({'Actual': y_test, 'Predicted ': RF_Prediction})
 RF_predicted_df.head(10)

"""#### XGB Classifier"""

from xgboost import XGBClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Assuming X and y are your features and labels
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=1002)

# Assuming 'fire' and 'not fire' are your classes in the target variable
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Assuming xgb_clf is your XGBoost classifier
xgb_clf = XGBClassifier()
xgb_clf.fit(X_train, y_train_encoded)

# Make predictions on the test set
y_pred_encoded = xgb_clf.predict(X_test)

# Convert predictions back to original class names
y_pred_original = label_encoder.inverse_transform(y_pred_encoded)

# Generate a classification report
class_report = classification_report(y_test, y_pred_original)
print(f"Classification Report is:\n{class_report}")

# Calculate and print the training score
training_score = xgb_clf.score(X_train, y_train_encoded) * 100
print(f"\nTraining Score:\n{training_score:.1f}")

XGB_Prediction = xgb_clf.predict(X_test)
XGB_predicted_df = pd.DataFrame({'Actual': y_test, 'Predicted ': XGB_Prediction})
XGB_predicted_df.head(10)